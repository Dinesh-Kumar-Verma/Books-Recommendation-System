<p align="center">
  <img src="https://img.shields.io/badge/ML-Recommendation%20System-blue" />
  <img src="https://img.shields.io/badge/Framework-Scikit--learn-orange" />
  <img src="https://img.shields.io/badge/Frontend-Streamlit-red" />
  <img src="https://img.shields.io/badge/Build-uv-green" />
  <img src="https://img.shields.io/badge/License-MIT-brightgreen" />
  <img src="https://img.shields.io/badge/Container-Docker-informational" />
  <img src="https://img.shields.io/badge/Python-3.10%2B-yellow" />
</p>

# ğŸ“š End-to-End Book Recommendation System

A production-ready, end-to-end book recommendation system built using collaborative filtering and K-Nearest Neighbors. The system processes raw book, user, and rating datasets, performs EDA, feature engineering, builds a recommendation engine, and serves real-time suggestions through a deployable Streamlit application.

---

## âœ¨ Features

- **Collaborative Filtering:** Recommends books based on the similarity of user ratings.
- **K-Nearest Neighbors (KNN):** Finds the closest books to a given book in the latent feature space.
- **Streamlit UI:** Interactive web interface for getting book recommendations.
- **Dockerized:** Comes with a `Dockerfile` for easy containerization and deployment.
- **End-to-End Pipeline:** A complete pipeline from data ingestion to model training and serving.

---

## ğŸ› ï¸ Tech Stack

- **Python:** The core programming language.
- **Pandas & NumPy:** For data manipulation and numerical operations.
- **Scikit-learn:** For building the recommendation model.
- **Streamlit:** For creating the web application.
- **Docker:** For containerization.
- **Gdown:** For downloading data from Google Drive.

---

## âš™ï¸ How It Works

The recommendation system is built as a multi-stage pipeline:

1.  **Data Ingestion:** ğŸ“¥ Downloads the dataset from a Google Drive link.
2.  **Data Validation & Cleaning:** ğŸ§¹
    -   Reads the raw `Books.csv`, `Users.csv`, and `Ratings.csv` files.
    -   Filters out users with less than 200 ratings and books with less than 50 ratings to reduce noise.
    -   Merges the datasets to create a final ratings dataset.
3.  **Data Transformation:** ğŸ”„
    -   Creates a user-item matrix (pivot table) where rows are book titles, columns are user IDs, and values are the ratings.
    -   Fills missing ratings with 0.
4.  **Model Training:** ğŸ§ 
    -   Converts the user-item matrix into a sparse matrix (`csr_matrix`) for efficiency.
    -   Trains a `NearestNeighbors` model with the `brute` force algorithm and `cosine` distance metric.
5.  **Recommendation:** ğŸ’¡
    -   Given a book, the model finds the `n` nearest books in the vector space.
    -   The Streamlit app displays the recommended books with their posters.

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ artifacts
â”‚   â”œâ”€â”€ dataset
â”‚   â”‚   â”œâ”€â”€ clean_data
â”‚   â”‚   â”œâ”€â”€ ingested_data
â”‚   â”‚   â”œâ”€â”€ raw_data
â”‚   â”‚   â””â”€â”€ transformed_data
â”‚   â”œâ”€â”€ serialized_objects
â”‚   â”‚   â”œâ”€â”€ book_names.pkl
â”‚   â”‚   â”œâ”€â”€ book_pivot.pkl
â”‚   â”‚   â””â”€â”€ final_rating.pkl
â”‚   â””â”€â”€ trained_model
â”‚       â””â”€â”€ model.pkl
â”œâ”€â”€ config
â”‚   â””â”€â”€ config.yaml
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ components
â”‚   â”‚   â”œâ”€â”€ stage_00_data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ stage_01_data_validation.py
â”‚   â”‚   â”œâ”€â”€ stage_02_data_transformation.py
â”‚   â”‚   â””â”€â”€ stage_03_model_trainer.py
â”‚   â”œâ”€â”€ config
â”‚   â”‚   â””â”€â”€ configuration.py
â”‚   â”œâ”€â”€ entity
â”‚   â”‚   â””â”€â”€ config_entity.py
â”‚   â”œâ”€â”€ exception
â”‚   â”‚   â””â”€â”€ exception_handler.py
â”‚   â”œâ”€â”€ logger
â”‚   â”‚   â””â”€â”€ log.py
â”‚   â”œâ”€â”€ pipeline
â”‚   â”‚   â””â”€â”€ training_pipeline.py
â”‚   â””â”€â”€ utils
â”‚       â””â”€â”€ util.py
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .gitignore
â”œâ”€â”€ app.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ template.py
```

-   **`artifacts`**: Stores all the data and model files generated by the pipeline.
-   **`config`**: Contains the configuration file for the project.
-   **`src`**: The main source code for the project.
    -   **`components`**: Contains the different stages of the ML pipeline.
    -   **`pipeline`**: The main training pipeline that orchestrates the components.
-   **`app.py`**: The Streamlit application for serving the recommendations.
-   **`main.py`**: The main script to run the training pipeline.
-   **`Dockerfile`**: For building the Docker image.

---

## ğŸš€ How to Run

### 1. Clone the repository

```bash
git clone https://github.com/your-username/Book-Recommendation-System.git
cd Book-Recommendation-System
```

### 2. Install dependencies

It is recommended to use a virtual environment.

```bash
uv venv
uv sync
```

### 3. Run the training pipeline

This will download the data, process it, and train the model.

```bash
python main.py
```

### 4. Run the Streamlit app

```bash
streamlit run app.py
```

Open your browser and go to `http://localhost:8501`.

---

## ğŸ“± How to Use the App

1.  Once the app is running, you will see a dropdown menu with a list of books.
2.  Select a book from the dropdown.
3.  Click the "Show Recommendation" button.
4.  The app will display 5 book recommendations with their posters.
5.  You can also retrain the model by clicking the "Train Recommender System" button.

---

## ğŸ³ Docker Deployment

You can also run the application using Docker.

### 1. Build the Docker image

```bash
docker build -t book-recommendation .
```

### 2. Run the Docker container

```bash
docker run -p 8501:8501 book-recommendation
```

The application will be available at `http://localhost:8501`.

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
